import tensorflow as tf
import os
from os.path import isfile, join
import numpy as np
import shutil
from tensorflow import keras
from pathlib import Path
from IPython.display import display, Audio 
import subprocess data_directory = "testing"
audio_folder= "audio"
noise_folder = "noises"
audio_path = os.path.join(data_directory, audio_folder) 
noise_path = os.path.join(data_directory, noise_folder) 
voicefile_names=os.listdir ("testing/audio")
for t in voicefile_names:
    p=os.listdir("testing/audio"+"/"+t)
    len_class=len(p)
    voice_files_count.append(len_class)
voice_files_count
noise_files_count=[] 
for t in noisefile_names:
    p=os.listdir("testing/noises"+"/"+t)
    len_class=len(p)
    noise_files_count.append(len_class)
noise_files_count
valid_split = 0.1 
shuffle_seed = 43
sample_rate= 16000
scale = 0.5
batch_size = 128
epochs = 15
noise_paths = []
for subdir in os.listdir (noise_path): 
    subdir_path = Path (noise_path) / subdir 
    if os.path.isdir (subdir_path):
        noise_paths += [
            os.path.join(subdir_path, filepath)
            for filepath in os.listdir(subdir_path) 
                if filepath.endswith(".wav")
        ]
audio_paths = []
for subdir in os.listdir (audio_path): 
    subdir_path = Path (audio_path) / subdir 
    if os.path.isdir (subdir_path):
        audio_paths += [
            os.path.join(subdir_path, filepath)
            for filepath in os.listdir(subdir_path) 
                if filepath.endswith(".wav")
        ]
import librosa
x, sr = librosa.load ('testing/noises/other/Oexercise_bike.wav')
print(x.shape)
print(sr)
#os.system(command)
def load_audio_sample(path):
    sample, sampling_rate = tf.audio.decode_wav(tf.io.read_file(path), desired_channels=1)
    
    #sample, sampling_rate= librosa.load (path, sr=16000)
    print("sampling rate of original audio", sampling_rate) 
    if sampling_rate == sample_rate:
        print("shape", sample.shape[0])
        slices = int(sample.shape[0] / sample_rate) 
        print(slices)
        sample = tf.split(sample[: slices * sample_rate], slices) 
        return sample
    else:
        print("Sampling rate for", path, "is incorrect")
        return None
    
audio = []
for path in audio_paths:
    sample = load_audio_sample(path)
    if sample:
        audio.extend(sample)

audio = tf.stack(audio)
#os.system(command)
def load_noise_sample(path):
    sample, sampling_rate = tf.audio.decode_wav(tf.io.read_file(path), desired_channels=1)
    
    #sample, sampling_rate= librosa.load (path, sr=16000)
    print("sampling rate of original audio", sampling_rate) 
    if sampling_rate == sample_rate:
        print("shape", sample.shape[0])
        slices = int(sample.shape[0] / sample_rate) 
        print(slices)
        sample = tf.split(sample[: slices * sample_rate], slices) 
        return sample
    else:
        print("Sampling rate for", path, "is incorrect")
        return None
    
noises = []
for path in noise_paths:
    sample = load_noise_sample(path)
    if sample:
        noises.extend(sample)

noises = tf.stack(noises)
##Dataset Generation
def paths_and_labels_to_dataset (audio_paths, labels): 
    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths) 
    audio_ds = path_ds.map(lambda x: path_to_audio(x)) 
    label_ds = tf.data.Dataset.from_tensor_slices(labels) 
    return tf.data.Dataset.zip((audio_ds, label_ds))
def path_to_audio (path):
    audio = tf.io.read_file(path)
    audio, _ = tf.audio.decode_wav (audio, 1, sample_rate) 
    return audio
##Add noise to Dataset
def add_noise (audio, noises=None, scale=0.5):
    if noises is not None:
        tf_rnd= tf.random.uniform(
            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32
        )
        noise = tf.gather (noises, tf_rnd, axis=0)
        prop= tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1) 
        prop= tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)

        audio = audio + noise * prop * scale
    return audio
def audio_to_fft (audio):
   audio = tf.squeeze (audio, axis=-1)
   fft = tf.signal.fft(tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64))
   fft = tf.expand_dims(fft, axis=-1)

   return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])

# Shuffle to generate random data 
rng = np.random.RandomState(shuffle_seed) 
rng.shuffle (audio_paths)
rng = np.random.RandomState(shuffle_seed) 
rng.shuffle (labels)
# Split into training and validation
num_val_samples = int(valid_split * len(audio_paths)) 
train_audio_paths = audio_paths[:-num_val_samples] 
train_labels = labels[:-num_val_samples]

valid_audio_paths = audio_paths[-num_val_samples:] 
valid_labels = labels[-num_val_samples:]
# Create datasets, one for training and the other for validation 
train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)
train_ds = train_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(batch_size)

valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)
valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=shuffle_seed).batch(32)
# Add noise to the training set
train_ds = train_ds.map(
    lambda x, y: (add_noise (x, noises, scale=scale), y), 
    num_parallel_calls=tf.data.experimental. AUTOTUNE,)

# Transform audio wave to the frequency domain using `audio_to_fft`
train_ds = train_ds.map(
    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)

train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)

valid_ds = valid_ds.map(
    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)

valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)
from tensorflow.keras.layers import Conv1D
def residual_block (x, filters, conv_num = 3, activation = "relu"): 
    s = keras.layers.Conv1D(filters, 1, padding = "same")(x)
    
    for i in range(conv_num - 1):
        x = keras.layers.Conv1D (filters, 3, padding= "same")(x) 
        x = keras.layers.Activation(activation)(x)

    x = keras.layers.Conv1D(filters, 3, padding = "same")(x) 
    x = keras.layers.Add()([x, s])
    x = keras.layers.Activation(activation)(x)
    
    return keras.layers.MaxPool1D(pool_size = 2, strides = 2)(x)

def build_model(input_shape, num_classes): 
    inputs = keras.layers.Input(shape = input_shape, name="input")
    # Add a Conv1D layer to process the input with 1 channel
    #x = keras.layers.Conv1D(filters=16, kernel_size=1, padding="same")(inputs)
    x = residual_block(inputs, 16, 2)
    x = residual_block(inputs, 32, 2)
    x = residual_block(inputs, 64, 3) 
    x = residual_block(inputs, 128, 3) 
    x = residual_block(inputs, 128, 3)
    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)
    #x = layers.GlobalAveragePooling1D()(x)  # Global Average Pooling
    x = keras.layers.Flatten()(x)
    x = keras.layers.Dense(256, activation="relu")(x)
    x = keras.layers.Dense (128, activation="relu")(x)
    

    outputs = keras.layers.Dense(num_classes, activation = "softmax", name = "output")(x)
    
    return keras.models.Model(inputs = inputs, outputs = outputs)

model = build_model((sample_rate // 2, 1), len(class_names))

model.summary()

model.compile(optimizer="Adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"]) 
model_save_filename="model.keras"

earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) 
mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(model_save_filename, monitor="val_accuracy", save_best_only=True)
epochs = 15
history = model.fit(
    train_ds,
    epochs = epochs,
    validation_data=valid_ds,
    callbacks=[earlystopping_cb, mdlcheckpoint_cb],
)
#Accuracy
print("Accuracy of model: ", model.evaluate(valid_ds))
#Predict
SAMPLES_TO_DISPLAY = 10
test_ds = paths_and_labels_to_dataset (valid_audio_paths, valid_labels)
test_ds = test_ds.shuffle(buffer_size=batch_size * 8, seed = shuffle_seed). batch( batch_size
)
test_ds = test_ds.map(lambda x, y: (add_noise (x, noises, scale=scale), y))

for audios, labels in test_ds.take(1):
    ffts = audio_to_fft(audios)
    y_pred = model.predict(ffts)
    rnd= np.random.randint(0, batch_size, SAMPLES_TO_DISPLAY) 
    audios = audios.numpy()[rnd, :, :] 
    labels = labels.numpy()[rnd]
    y_pred = np.argmax(y_pred, axis=-1) [rnd]

    for index in range(SAMPLES_TO_DISPLAY):
        print(
            "Speaker:\33{} {}\33[0m\tPredicted: \33{} {}\33[0m".format(
                 "[92m" if labels[index] == y_pred[index] else "[91m", 
                 class_names[labels[index]],
                "[92m" if labels[index] == y_pred[index] else "[91m", 
                class_names [y_pred[index]],
            )
        )
        if labels[index] ==y_pred[index]:
            print("Welcome")
        else:
            print("Sorry")
        print("The speaker is" if labels[index] == y_pred[index] else "", class_names[y_pred[index]])
#Predict Speaker from the test dataset for real time pred.
def paths_to_dataset (audio_paths):
    path_ds = tf.data.Dataset.from_tensor_slices (audio_paths) 
    return tf.data.Dataset.zip((path_ds))

def predict (path, labels):
    test = paths_and_labels_to_dataset (path, labels)

    test = test.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed). batch( batch_size)
    test = test.prefetch(tf.data.experimental.AUTOTUNE)

    test = test.map(lambda x, y: (add_noise (x, noises, scale=scale), y))

    for audios, labels in test. take(1):
        ffts = audio_to_fft (audios)
        y_pred = model.predict(ffts)
        rnd = np.random.randint(0, 1, 1) 
        audios = audios.numpy()[rnd, :]
        labels = labels.numpy()[rnd]
        y_pred= np.argmax(y_pred, axis=-1)[rnd]

    for index in range(1):
        print(
        "Speaker:\33{} {}\33[0m\tPredicted:\33{} {}\33[0m".format(
        "[92m",y_pred[index],
            "[92m", y_pred[index]
          )
        )
        print("Speaker Predicted:",class_names[y_pred[index]])
path = ["testing/audio/speaker8/resampled_speaker8_265.wav"] 
labels = ["unknown"]
try:
    predict (path, labels)
except:
    print("Error! Check if the file correctly passed or not!")
